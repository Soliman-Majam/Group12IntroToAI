{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0f573de429d8ed4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We did testing on importing the different datasets to see that there are no errors with importing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fa876bd378eeb56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # reading gym members dataset\n",
    "# df = pd.read_csv('gym_members_exercise_tracking.csv', na_values=['NA', '?'])\n",
    "# \n",
    "# # print first 5 rows to check it works\n",
    "# print(df[:5])\n",
    "# print(\" \")\n",
    "# \n",
    "# # checking to see if there is any missing data\n",
    "# print(df.isnull().any())\n",
    "# print(\" \")\n",
    "# ============================================================================="
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e4aa80d429e66e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#Treisis Dataset:\n",
    "#df = pd.read_csv('housing_price_dataset.csv', na_values=['NA', '?'])\n",
    "\n",
    "#print 5 rows\n",
    "\n",
    "#print(df[:5])\n",
    "#print(\" \")\n",
    "\n",
    "#checking for missing data\n",
    "\n",
    "#print(df.isnull().any())\n",
    "#------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5c5b04682c967c60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fatima Dataset:\n",
    "\n",
    "# Load the dataset and handle NA values\n",
    "#df = pd.read_csv('heart_attack_analysis.csv', na_values=['NA', '?'])\n",
    "\n",
    "# Print the entire DataFrame\n",
    "#pd.set_option('display.max_rows', None)  # all rows\n",
    "#print(df)\n",
    "#print(\" \")\n",
    "\n",
    "# Checking for missing data in all columns\n",
    "#print(\"Missing data in each column:\")\n",
    "#print(df.isnull().any())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3b436031ac852290"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally settled on the mushroom dataset and loaded it in"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2240fa5a1ee7008a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset and handle NA values\n",
    "df = pd.read_csv('mushroom_cleaned.csv', na_values=['NA', '?'])\n",
    "\n",
    "# Print the entire DataFrame\n",
    "pd.set_option('display.max_rows', None)  # all rows\n",
    "print(df)\n",
    "print(\" \")\n",
    "\n",
    "# Checking for missing data in all columns\n",
    "print(\"Missing data in each column:\")\n",
    "print(df.isnull().any())\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f89d4fe964231d75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Printed the entire dataset just to be sure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd0dbbdeb25efe2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the entire DataFrame\n",
    "pd.set_option('display.max_rows', None)  # all rows\n",
    "print(df)\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8a2e99b39629dd99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking to see if there is any null data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4439cb60778d954"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking for missing data in all columns\n",
    "print(\"Missing data in each column:\")\n",
    "print(df.isnull().any())\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ab49c975b566a0a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There was none, so we now we collected all the names of the non-target features and make sure they match\n",
    "up with the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed01348e2f49c63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collecting  the collumn names for non-target features\n",
    "result = []\n",
    "for x in df.columns:\n",
    "    if x !='class':\n",
    "        result.append(x)\n",
    "\n",
    "print(\"\\nFeature names: \")        \n",
    "print(result)       "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "170b5d1b538e5fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "They match! So now we assign the feature and target data and print the shape of\n",
    "our original data to see if it later matches up to the test and train data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3588d8e2b33c5566"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining feature and target data\n",
    "X = df[result].values\n",
    "y= df['class'].values\n",
    "\n",
    "# Print the shape of the original data\n",
    "print(\"\\nOriginal dataset shape:\")\n",
    "print(\"Features (X):\", X.shape)\n",
    "print(\"Target (y):\", y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c07b7af73cbb6181"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin the train_test split with a starting test size of 20% and a random state of 7\n",
    "as everyone likes the number 7"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9748edceefe8fa2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "978b3c1e83dbe17c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we print the sizes of the training and testing sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756cf315fda91622"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the shape of the split data\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"Training set - Features (X_train):\", X_train.shape, \"Target (y_train):\", y_train.shape)\n",
    "print(\"Testing set - Features (X_test):\", X_test.shape, \"Target (y_test):\", y_test.shape)\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9a24a0a190098a40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "They match up which is great, now we prepare the standard scaler to use on our data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3efe841eaad2a58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preparing the standard scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "print(\"printing training set\")\n",
    "print(X_train[0])\n",
    "print(X_train_std[0])\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "44a3096466a43e7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we implement perceptron model, fit it to the data and make a prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd4152d55c29ea11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creating perceptron model\n",
    "per = Perceptron(max_iter=35,tol=0.001,eta0=1)\n",
    "\n",
    "# training the model with training data (scaled by standard scaler) and training target\n",
    "per.fit(X_train_std,y_train)\n",
    "\n",
    "# making prediction for the test data\n",
    "pred = per.predict(X_test_std)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ad3a342795f5e9d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we print the accuracy score from that prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10138f7728b0196e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check accuracy with accuracy score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, pred))\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7b5ffb61ddd0cfbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy is very low (48%) so we have work to do, but first we will plot out a confusion matrix of \n",
    "those results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dea4e625be1c3db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display confusion matrix as a plot\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=per.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8cc4cebd439767b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly enough, if we don't use the standard scaler our accuracy is higher"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81665cf7332c042e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per = Perceptron(max_iter=35,tol=0.001,eta0=1)\n",
    "per.fit(X_train, y_train)\n",
    "pred = per.predict(X_test)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, pred))\n",
    "print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e376cd9a72ecdc59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We investigate more and test the boundaries of the split and how far we can go"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a70f16a2ac2d6a9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "# accuracy score is 0.48\n",
    "\n",
    "#test with a higher percentage 0.25\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "# accuracy is 0.58\n",
    "\n",
    "#test with a higher percentage 0.3\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "# accuracy is 0.58\n",
    "\n",
    "#test with a higher percentage 0.35\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=7)\n",
    "# accuracy is 0.62"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d4a70280f4eefa77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turns out having the test size rise to 35% greatly increases the accuracy from 0.48 to 0.62.\n",
    "So now let's repeat the process with this split. First we add some libraries that we consider using later"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae939b1e0a500112"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f4191c8e2ec9d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now let's go through the procedure using the new split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bdba8d8bfef613d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test with a higher percentage 0.35\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=7)\n",
    "# accuracy is 0.62\n",
    "\n",
    "\n",
    "# Print the shape of the split data\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"Training set - Features (X_train):\", X_train.shape, \"Target (y_train):\", y_train.shape)\n",
    "print(\"Testing set - Features (X_test):\", X_test.shape, \"Target (y_test):\", y_test.shape)\n",
    "print(\" \")\n",
    "\n",
    "# preparing the standard scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "print(\"printing training set\")\n",
    "print(X_train[0])\n",
    "print(X_train_std[0])\n",
    "print(\" \")\n",
    "\n",
    "# not using scaler yield higher accuracy score?\n",
    "# =============================================================================\n",
    "# per = Perceptron(max_iter=35,tol=0.001,eta0=1)\n",
    "# per.fit(X_train, y_train)\n",
    "# pred0 = per.predict(X_test)\n",
    "# print('Accuracy: %.2f' % accuracy_score(y_test, pred0))\n",
    "# print(\" \")\n",
    "# =============================================================================\n",
    "\n",
    "# creating perceptron model\n",
    "per = Perceptron(max_iter=35,tol=0.001,eta0=1)\n",
    "\n",
    "# training the model with training data (scaled by standard scaler) and training target\n",
    "per.fit(X_train_std,y_train)\n",
    "\n",
    "# making prediction for the test data\n",
    "pred = per.predict(X_test_std)\n",
    "\n",
    "# check accuracy with accuracy score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, pred))\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display confusion matrix as a plot\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=per.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f140107b6c521512"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's at this point we find out about the classification report, which will be a powerful tool\n",
    "for us to monitor our progress. It lists more statistics than just accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6b7f91b8c1ba2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "\n",
    "report = classification_report(y_test, pred) \n",
    "\n",
    "print(report) "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e6e5a0132d13330"
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the mushroom dataset looked promising, it was also extremely similar to a dataset used in tutorial\n",
    "5, which we didn't initially realise. Just to be safe we decided it would be best to change it for\n",
    "a different dataset. We found one we wanted to do which is measuring the potability of water\n",
    "based on a variety of features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e499b15551f255fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset and handle NA values\n",
    "df = pd.read_csv('waterpotability.csv', na_values=['NA', '?'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "48f7e9cebdd01d77"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
